The article argues that computer science is not a science. Science relies on a theory that predicts observation. An experiment always relies on empirical evidence. Near the start of computer science, this was the case, since researchers depended much more on fixed metrics like run time, memory used, etc. The diversification of hardware has made collecting those metrics less useful/applicable. There's no point in knowing exactly how long your program takes to run on a specific machine, since it's going to run on a hundred different machines with different levels of compute.

As a result, computer science become more similar to philosophy, with computer scientists conjecturing about the best way to complete a task. The basis of these conjectures are abstract systems (like big O notation) that allow you to "prove" that an algorithm is better than another.

What the article said is very profound, and I'm happy I was assigned to read it. It says a lot about how we should think about programming. However, I do have one disagreement. I believe that empirical experiments *are* conducted based on the work of computer scientists, just not *by* computer scientists. That experimentation happens after a program is finished and shipped to a beta tester (or even an end user, given they have some way of communicating with the developer). 

After a program is finished, and distributed to beta testers, many, MANY different experiments begin. The output of those experiments are what plays out on the beta testers' screens. The "hypothesis" was the software's plan, if it had a plan, otherwise, the invisible plan in the developer's brain. *"If I run this code, then (thing) will happen, without any errors."* Occasionally, this hypothesis is explicitly and quantifiably broken, like if a test fails or if the program crashes. But a lot of the time, an issue that pops up isn't a test failing, and is more difficult to quantify. That's why I agree that computer science differs from other empirical sciences.